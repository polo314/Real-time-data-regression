from __future__ import division
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import datetime
import requests


def print_full(x):
    pd.set_option('display.max_rows', len(x))
    print(x)
    pd.reset_option('display.max_rows')

def print_full_columns(x):
    pd.set_option('display.max_columns', len(x.columns.values))
    print(x)
    pd.reset_option('display.max_columns')

def slope(data,attribute,length):
    data['slope'+str(length)+'_'+str(attribute)]=pd.Series()
    ind=0
    while math.isnan(data[str(attribute)][ind])==True:
        ind+=1
    for i in range(0+length-1+ind,len(data)):
        ratio= (data[attribute][i]-data[attribute][i-length+1])/length
        data.loc[i,'slope'+str(length)+'_'+str(attribute)]= ratio
    return 'slope'+str(length)+'_'+str(attribute)

def slope_cycle(data,attribute,length,size):

    ind=0
    while math.isnan(data[str(attribute)][ind])==True:
        ind+=1
    if size != len(data):
            for i in range((len(data)-(size)),len(data)):
                ratio= (data[attribute][i]-data[attribute][i-length+1])/length
                data.loc[i,'slope'+str(length)+'_'+str(attribute)]= ratio
    return 'slope'+str(length)+'_'+str(attribute)

def msroot(data,attribute,movavg_attribute,movavg_window):
    data['dif'+str(movavg_window)+'_'+str(attribute)]=pd.Series()
    data['msq'+str(movavg_window)+'_'+str(attribute)]=pd.Series()
    a=int(movavg_window/2)
    total=0
    ind1=0
    while math.isnan(data[movavg_attribute][ind1])==True:
        ind1+=1
    ind2=0
    while math.isnan(data[attribute][ind2])==True:
        ind2+=1
    ind=max(ind1,ind2)
    for i in range(movavg_window+ind-1,len(data)):
        dif=data[str(movavg_attribute)][i]-data[attribute][i-a]
        data.loc[i,'dif'+str(movavg_window)+'_'+str(attribute)]=dif**2
    for j in range(movavg_window+ind-1,len(data)):
        total+=data['dif'+str(movavg_window)+'_'+str(attribute)][j]
        divid=j-movavg_window+2-ind
        data.loc[j,'msq'+str(movavg_window)+'_'+str(attribute)]=math.sqrt(total/divid)
    return 'msq'+str(movavg_window)+'_'+str(attribute)

def movavg(data,attribute,movavg_window):
    data['movavg'+str(movavg_window)+'_'+str(attribute)]=pd.Series()
    ind=0
    while math.isnan(data[attribute][ind])==True:
        ind+=1
    for i in range(movavg_window-1+ind,len(data)):
        counter=0
        for j in range(i-movavg_window+1,i+1):
            counter+=data[attribute][j]
        data.loc[i,'movavg'+str(movavg_window)+'_'+str(attribute)]=counter/movavg_window
    return 'movavg'+str(movavg_window)+'_'+str(attribute)

def movavg_cycle(data,attribute,movavg_window,size):
    if size !=len(data):
        for i in range(len(data)-size,len(data)):
            counter=0
            for j in range(i-movavg_window+1,i+1):
                counter+=data[attribute][j]
            data1=data['movavg'+str(movavg_window)+'_'+str(attribute)].copy()
            data1.loc[i]=counter/movavg_window
            data.loc[i,'movavg'+str(movavg_window)+'_'+str(attribute)]=data1[i]
    return 'movavg'+str(movavg_window)+'_'+str(attribute)

def movavg_retro_cycle(data,attribute,movavg_window,size):
    ind=0
    while math.isnan(data[attribute][ind])==True:
        ind+=1
    if size!=len(data):
        for i in range(len(data)-(size+1),len(data)-1):
            counter=0
            for j in range(i-movavg_window+1,i+1):
                counter+=data[attribute][j]
            data.loc[i+1,'movavg_retro'+str(movavg_window)+'_'+str(attribute)]=counter/movavg_window
    else:
        data['movavg_retro'+str(movavg_window)+'_'+str(attribute)]=pd.Series()
        for i in range(movavg_window-1+ind,len(data)-1):
            counter=0
            for j in range(i-movavg_window+1,i+1):
                counter+=data[attribute][j]
            data.loc[i+1,'movavg_retro'+str(movavg_window)+'_'+str(attribute)]=counter/movavg_window
    return 'movavg_retro'+str(movavg_window)+'_'+str(attribute)

def movavg_retro(data,attribute,movavg_window):
    data['movavg_retro'+str(movavg_window)+'_'+str(attribute)]=pd.Series()
    ind=0
    while math.isnan(data[attribute][ind])==True:
        ind+=1
    for i in range(movavg_window-1+ind,len(data)-1):
        counter=0
        for j in range(i-movavg_window+1,i+1):
            counter+=data[attribute][j]
        data.loc[i+1,'movavg_retro'+str(movavg_window)+'_'+str(attribute)]=counter/movavg_window
    return 'movavg_retro'+str(movavg_window)+'_'+str(attribute)

def local_msq(data,attribute,attribute1,msq_win):
    data['local_msq'+str(msq_win)+'_'+str(attribute)]=pd.Series()
    ind1=0
    while math.isnan(data[attribute1][ind1])==True:
        ind1+=1
    ind2=0
    while math.isnan(data[attribute][ind2])==True:
        ind2+=1
    ind=max(ind1,ind2)
    for i in range(ind,len(data)-msq_win+1):
        local_msq=0
        for j in range(i,i+msq_win):
            local_msq+=(data[str(attribute)][j]- data[str(attribute1)][j])**2
        data.loc[i+msq_win-1,'local_msq'+str(msq_win)+'_'+str(attribute)]=math.sqrt(local_msq/msq_win)
    return 'local_msq'+str(msq_win)+'_'+str(attribute)

def local_msq_cycle(data,attribute,attribute1,msq_win,size):

    ind1=0
    while math.isnan(data[attribute1][ind1])==True:
        ind1+=1
    ind2=0
    while math.isnan(data[attribute][ind2])==True:
        ind2+=1
    if size!= len(data):
        for i in range(len(data)-size-(msq_win+1),len(data)-msq_win+1):
            local_msq=0
            for j in range(i,i+msq_win):
                local_msq+=(data[str(attribute)][j]- data[str(attribute1)][j])**2
            data.loc[i+msq_win-1,'local_msq'+str(msq_win)+'_'+str(attribute)]=math.sqrt(local_msq/msq_win)
    return 'local_msq'+str(msq_win)+'_'+str(attribute)

def entropy(data,attribute,msq_attribute,movavg_window,size):
    if size == len(data):
        data['entropy_source'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=pd.Series()
        data['entropy'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=pd.Series()
        data['pr1_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=pd.Series()
        data['pr2_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=pd.Series()
        data['pr3_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=pd.Series()
        data['variation'+'_'+'_'+str(msq_attribute)+str(attribute)]=pd.Series()
    base=math.e
    ind=0
    while math.isnan(data[str(msq_attribute)][ind])==True:
        ind+=1
    count1=math.log(3,base)/3
    count2=math.log(3,base)/3
    count3=math.log(3,base)/3
    for i in range(ind+1,len(data)):
        variation=data[str(attribute)][i]-data[str(attribute)][i-1]
        data.loc[i,'variation'+'_'+'_'+str(msq_attribute)+str(attribute)]=abs(variation)
        sum=count1+count2+count3
        normalization=1
        thr1=1*data[str(msq_attribute)][i]
        thr2=2*data[str(msq_attribute)][i]
        if abs(variation) <= thr1:
           count1+=1
           sum=count1+count2+count3
           pr1=float(count1)/sum
           pr2=float(count2)/sum
           pr3=float(count3)/sum
           entropy= -pr1*math.log(pr1,base)
           data.loc[i,'entropy'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]= entropy
           data.loc[i,'pr1_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr1*math.log(pr1,base)/normalization)
           data.loc[i,'pr2_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr2*math.log(pr2,base)/normalization)
           data.loc[i,'pr3_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr3*math.log(pr3,base)/normalization)
        elif thr1 < abs(variation) <= thr2:
           count2+=1
           sum=count1+count2+count3
           pr1=float(count1)/sum
           pr2=float(count2)/sum
           pr3=float(count3)/sum
           entropy= -pr2*math.log(pr2,base)
           data.loc[i,'entropy'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]= entropy
           data.loc[i,'pr1_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr1*math.log(pr1,base)/normalization)
           data.loc[i,'pr2_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr2*math.log(pr2,base)/normalization)
           data.loc[i,'pr3_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr3*math.log(pr3,base)/normalization)
        else:
           count3+=1
           sum=count1+count2+count3
           pr1=float(count1)/sum
           pr2=float(count2)/sum
           pr3=float(count3)/sum
           entropy= -pr3*math.log(pr3,base)
           data.loc[i,'entropy'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]= entropy
           data.loc[i,'pr1_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr1*math.log(pr1,base)/normalization)
           data.loc[i,'pr2_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr2*math.log(pr2,base)/normalization)
           data.loc[i,'pr3_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr3*math.log(pr3,base)/normalization)
        entropy_s=-pr1*math.log(pr1,base)-pr2*math.log(pr2,base)-pr3*math.log(pr3,base)
        data.loc[i,'entropy_source'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=entropy_s/3

    return ['entropy_source'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute),
            'entropy'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute),
            ]

def entropy_cycle(data,attribute,msq_attribute,movavg_window,size):
    data['entropy_source'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=pd.Series()
    data['entropy'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=pd.Series()
    data['pr1_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=pd.Series()
    data['pr2_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=pd.Series()
    data['pr3_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=pd.Series()
    data['variation'+'_'+'_'+str(msq_attribute)+str(attribute)]=pd.Series()
    base=math.e
    ind=0
    while math.isnan(data[str(msq_attribute)][ind])==True:
        ind+=1
    count1=math.log(3,base)/3
    count2=math.log(3,base)/3
    count3=math.log(3,base)/3
    for i in range((len(data)-(size-1)),len(data)):
        variation=data[str(attribute)][i]-data[str(attribute)][i-1]
        data.loc[i,'variation'+'_'+'_'+str(msq_attribute)+str(attribute)]=abs(variation)
        sum=count1+count2+count3
        normalization=1#-math.log(pr1,base)-math.log(pr2,base)-math.log(pr3,base)
        thr1=1*data[str(msq_attribute)][i]
        thr2=2*data[str(msq_attribute)][i]
        if abs(variation) <= thr1:
           count1+=1
           sum=count1+count2+count3
           pr1=float(count1)/sum
           pr2=float(count2)/sum
           pr3=float(count3)/sum
           entropy= -pr1*math.log(pr1,base)
           data.loc[i,'entropy'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]= entropy
           data.loc[i,'pr1_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr1*math.log(pr1,base)/normalization)
           data.loc[i,'pr2_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr2*math.log(pr2,base)/normalization)
           data.loc[i,'pr3_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr3*math.log(pr3,base)/normalization)
        elif thr1 < abs(variation) <= thr2:
           count2+=1
           sum=count1+count2+count3
           pr1=float(count1)/sum
           pr2=float(count2)/sum
           pr3=float(count3)/sum
           entropy= -pr2*math.log(pr2,base)
           data.loc[i,'entropy'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]= entropy
           data.loc[i,'pr1_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr1*math.log(pr1,base)/normalization)
           data.loc[i,'pr2_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr2*math.log(pr2,base)/normalization)
           data.loc[i,'pr3_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr3*math.log(pr3,base)/normalization)
        else:
           count3+=1
           sum=count1+count2+count3
           pr1=float(count1)/sum
           pr2=float(count2)/sum
           pr3=float(count3)/sum
           entropy= -pr3*math.log(pr3,base)
           data.loc[i,'entropy'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]= entropy
           data.loc[i,'pr1_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr1*math.log(pr1,base)/normalization)
           data.loc[i,'pr2_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr2*math.log(pr2,base)/normalization)
           data.loc[i,'pr3_'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=(-pr3*math.log(pr3,base)/normalization)
        entropy_s=-pr1*math.log(pr1,base)-pr2*math.log(pr2,base)-pr3*math.log(pr3,base)
        data.loc[i,'entropy_source'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute)]=entropy_s/3

    return ['entropy_source'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute),
            'entropy'+str(movavg_window)+'_'+str(msq_attribute)+'_'+str(attribute),
            ]

def total_movavg(data,attribute,*args):
    set=args
    dividend=0
    data['totalmovavg'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]=0
    for j in range(1,len(set)+1):
        dividend+=j
    for n in range(0,len(set)):
        average_function=movavg(data,attribute,set[n])
        data['totalmovavg'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]+=(len(set)-n)*data[average_function]
    data['totalmovavg'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]=data['totalmovavg'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]/dividend
    return 'totalmovavg' + str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])

def total_movavg_cycle(data,attribute,size,*args):
    set=args
    dividend=0
    for j in range(1,len(set)+1):
        dividend+=j
    for n in range(0,len(set)):
        average_function=movavg_cycle(data,attribute,set[n],size)
        for q in range(len(data)-(size),len(data)):
            if n==0:
                data.loc[q,'totalmovavg'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]=0
            data.loc[q,'totalmovavg'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]+=(len(set)-n)*data[average_function][q]
    for q in range(len(data)-(size),len(data)):
        data['totalmovavg'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])][q]=data['totalmovavg'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])][q]/dividend
    return 'totalmovavg' + str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])

def total_movavg_predict_cycle(data,attribute,size,*args):
    set=args
    dividend=0
    for j in range(1,len(set)+1):
        dividend+=j
    for n in range(0,len(set)):
        average_function=movavg_retro_cycle(data,attribute,set[n],size)
        for q in range(len(data)-(size),len(data)):
            if n==0:
                data.loc[q,'totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]=0
            data.loc[q,'totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]+=(len(set)-n)*data[average_function][q]
        del data[average_function]
    for q in range(len(data)-(size),len(data)):
        data['totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])][q]=data['totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])][q]/dividend
    for i in range(0-(size)+len(data)-int(round((set[0]/2)+1)),len(data)-int(round((set[0]/2)+1))):
        data.loc[i,'totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]=data['totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])][i+int(round((set[0]/2)+1))]
    for i in range(len(data)-int(round((set[0]/2)+1)),len(data)):
        data.loc[i,'totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]=np.nan
    return 'totalmovavg_predict' + str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])

def total_movavg_predict(data,attribute,*args):
    set=args
    dividend=0
    data['totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]=0
    for j in range(1,len(set)+1):
        dividend+=j
    for n in range(0,len(set)):
        average_function=movavg_retro(data,attribute,set[n])
        data['totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]+=(len(set)-n)*data[average_function]
        del data[average_function]
    data['totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]=data['totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]/dividend
    for i in range(0,len(data)-int(round((set[0]/2)+1))):
        data.loc[i,'totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]=data['totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])][i+int(round((set[0]/2)+1))]
    for i in range(len(data)-int(round((set[0]/2)+1)),len(data)):
        data.loc[i,'totalmovavg_predict'+ str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])]=np.nan
    return 'totalmovavg_predict' + str(attribute)+ str(set[0])+'_'+ str(set[len(set)-1])

def clean_labels(set_copy,attribute,loopset):

    for i in range(0,len(loopset)):
        short_window=loopset[i]
        window5=15*loopset[i]
        del set_copy['totalmovavg_predict'+attribute+str(short_window)+'_'+str(window5)]

def labels(trainset_copy,trainset,attribute,loopset):

    for i in range(0,len(loopset)):
        short_window=loopset[i]
        window5=15*loopset[i]
        trainset_copy['totalmovavg_predict'+attribute+str(short_window)+'_'+str(window5)]=trainset['totalmovavg_predict'+attribute+str(short_window)+'_'+str(window5)]



def splitting_cleaning(data,attribute,ceil_attribute,loopset,test_size,test_sizes1,layers):
    data[ceil_attribute+'_category']=np.ceil((abs(data[ceil_attribute])+0.001)/0.76) #0.76
    data[ceil_attribute+'_category'].where(data[ceil_attribute+'_category']<6,6.0,inplace=True)
    print_full(data[ceil_attribute+'_category'])
    from sklearn.model_selection import StratifiedShuffleSplit
    split=StratifiedShuffleSplit(n_splits=1,test_size=test_size,random_state=42)
    for train_index, test_index in split.split(data,data[ceil_attribute+'_category']):
        strat_train_set=data.loc[train_index]
        strat_test_set=data.loc[test_index]
    for set_ in (strat_train_set,strat_test_set):
        set_.drop(ceil_attribute+'_category',axis=1,inplace=True)


    test=strat_test_set.copy()
    test_labels=pd.DataFrame()
    clean_labels(test,attribute,loopset)
    labels(test_labels,strat_test_set,attribute,loopset)

    strat_train_set_layer={}
    strat_test_set_layer={}
    train={}
    train_labels={}
    test_layer={}
    test_layer_labels={}
    i=0
    while i<(layers-1):
        if i==0:
            strat_train_set[ceil_attribute+'_category']=np.ceil((abs(strat_train_set[ceil_attribute])+0.001)/0.76) #0.76
            strat_train_set[ceil_attribute+'_category'].where(strat_train_set[ceil_attribute+'_category']<6,6.0,inplace=True)
            split=StratifiedShuffleSplit(n_splits=1,test_size=test_sizes1,random_state=42)
            for train_index, test_index in split.split(strat_train_set,strat_train_set[ceil_attribute+'_category']):

                strat_train_set_layer['{0}'.format(i)]=data.loc[train_index]
                strat_test_set_layer['{0}'.format(i)]=data.loc[test_index]
            for set_ in (strat_train_set_layer['{0}'.format(i)],strat_test_set_layer['{0}'.format(i)]):
                set_.drop(ceil_attribute+'_category',axis=1,inplace=True)
            train['{0}'.format(i)]=strat_train_set_layer['{0}'.format(i)].copy()
            clean_labels(train['{0}'.format(i)],attribute,loopset)
            train_labels['{0}'.format(i)]={}
            labels(train_labels['{0}'.format(i)],strat_train_set_layer['{0}'.format(i)],attribute,loopset)
            i+=1


        else:
            strat_test_set_layer['{0}'.format(i-1)][ceil_attribute+'_category']=np.ceil((abs(strat_test_set_layer['{0}'.format(i-1)][ceil_attribute])+0.001)/0.76)
            strat_test_set_layer['{0}'.format(i-1)][ceil_attribute+'_category'].where(strat_test_set_layer['{0}'.format(i-1)][ceil_attribute+'_category']<6,6.0,inplace=True)
            size=0.01*int(100*((layers-1)-i)/((layers-1)-i+1))
            split=StratifiedShuffleSplit(n_splits=1,test_size=size,random_state=42)
            for train_index, test_index in split.split(strat_test_set_layer['{0}'.format(i-1)],strat_test_set_layer['{0}'.format(i-1)][ceil_attribute+'_category']):
                strat_train_set_layer['{0}'.format(i)]=data.loc[train_index]
                strat_test_set_layer['{0}'.format(i)]=data.loc[test_index]
            for set_ in (strat_train_set_layer['{0}'.format(i)],strat_test_set_layer['{0}'.format(i)]):
                set_.drop(ceil_attribute+'_category',axis=1,inplace=True)

            train['{0}'.format(i)]=strat_train_set_layer['{0}'.format(i)].copy()
            clean_labels(train['{0}'.format(i)],attribute,loopset)
            train_labels['{0}'.format(i)]={}
            labels(train_labels['{0}'.format(i)],strat_train_set_layer['{0}'.format(i)],attribute,loopset)
            if i==(layers-2):
                test_layer=strat_test_set_layer['{0}'.format(i)].copy()
                clean_labels(test_layer,attribute,loopset)
                labels(test_layer_labels,strat_test_set_layer['{0}'.format(i)],attribute,loopset)
            i+=1

    ret=[test_layer,test_layer_labels,test,test_labels]
    for i in range(0,layers-1):
        ret.append(train[str(i)])
        ret.append(train_labels[str(i)])
    return ret


def blender1(train_data,train_labels,list,wind):
    dataframe1=pd.DataFrame()
    dataframe2=pd.DataFrame()
    for i in range(0,len(list)):
        name=list[i]

        from sklearn.externals import joblib

        gbrt=joblib.load('gbrt_'+name)
        ada=joblib.load('ada_'+name)

        gbrt1=gbrt.predict(train_data)
        ada1=ada.predict(train_data)

        stacking=pd.DataFrame({'gbrt':gbrt1,'ada':ada1})

        forest_final=joblib.load(name)
        predictions_final=forest_final.predict(stacking)

        train_labels1=train_labels.copy()
        train_labels1=train_labels1[name].values
        final0=pd.DataFrame(predictions_final)
        finalg=pd.DataFrame(gbrt1)
        finala=pd.DataFrame(ada1)
        final1=pd.DataFrame(train_labels1)
        final0=final0.reset_index()
        finalg=finalg.reset_index()
        finala=finala.reset_index()
        final1=final1.reset_index()
        dataframe=pd.concat([final1,final0],axis=1)
        del dataframe['index']
        dataframe=dataframe.values
        dataframe=pd.DataFrame(dataframe,columns=[name+wind,'predictions_'+name+wind])
        dataframe1=pd.concat([dataframe1,dataframe],axis=1)
        dataframe2=pd.concat([finalg,finala],axis=1)
        del dataframe2['index']
        dataframe2=dataframe2.values
        dataframe2=pd.DataFrame(dataframe2,columns=['predictions_gbrt_'+name+wind,'predictions_ada_'+name+wind])
        dataframe1=pd.concat([dataframe1,dataframe2],axis=1)


    return dataframe1


def predictor_one(data,data_labels,data_iterator,starters):
    valori=data_labels.columns.values

    for q in starters:
        for h in range(0,len(valori)):
            name=valori[h]
            if math.isnan(data[name+str(starters.index(q))][0])==False:
                    data.loc[0,'pred_dif']=(data['predictions_'+name+str(starters.index(q))][0]-data[name+str(starters.index(q))][0])**2
                    data.loc[0,'gbrt_dif']=(data['predictions_gbrt_'+name+str(starters.index(q))][0]-data[name+str(starters.index(q))][0])**2
                    data.loc[0,'ada_dif']=(data['predictions_ada_'+name+str(starters.index(q))][0]-data[name+str(starters.index(q))][0])**2

                    weight_pred=data_iterator['variance_pred'+str(starters.index(q))+name][0]
                    iteration=data_iterator['counter'+str(starters.index(q))+name][0]
                    total_pred=(((weight_pred)**2)*iteration+data['pred_dif'][0])/(iteration+1)
                    total_pred=math.sqrt(total_pred)
                    data_iterator.loc[0,'variance_pred'+str(starters.index(q))+name]=total_pred
                    weight_gbrt=data_iterator['variance_gbrt'+str(starters.index(q))+name][0]
                    total_gbrt=(((weight_gbrt)**2)*iteration+data['gbrt_dif'][0])/(iteration+1)
                    total_gbrt=math.sqrt(total_gbrt)
                    data_iterator.loc[0,'variance_gbrt'+str(starters.index(q))+name]=total_gbrt
                    weight_ada=data_iterator['variance_ada'+str(starters.index(q))+name][0]
                    total_ada=(((weight_ada)**2)*iteration+data['ada_dif'][0])/(iteration+1)
                    total_ada=math.sqrt(total_ada)
                    data_iterator.loc[0,'variance_ada'+str(starters.index(q))+name]=total_ada
                    total_sum=data_iterator['variance_pred'+str(starters.index(q))+name][0]+data_iterator['variance_gbrt'+str(starters.index(q))+name][0]+data_iterator['variance_ada'+str(starters.index(q))+name][0]
                    p_pred=total_sum/data_iterator['variance_pred'+str(starters.index(q))+name][0]
                    p_gbrt=total_sum/data_iterator['variance_gbrt'+str(starters.index(q))+name][0]
                    p_ada=total_sum/data_iterator['variance_ada'+str(starters.index(q))+name][0]
                    dividend=p_pred+p_gbrt+p_ada
                    data.loc[0,'prediction_final_'+name+str(starters.index(q))]=(p_pred*data['predictions_'+name+str(starters.index(q))][0]+p_gbrt*data['predictions_gbrt_'+name+str(starters.index(q))][0]+p_ada*data['predictions_ada_'+name+str(starters.index(q))][0])/dividend
                    data_iterator.loc[0,'counter'+str(starters.index(q))+name]=data_iterator['counter'+str(starters.index(q))+name][0]+1
            if math.isnan(data[name+str(starters.index(q))][0])==True:
                    total_sum=data_iterator['variance_pred'+str(starters.index(q))+name][0]+data_iterator['variance_gbrt'+str(starters.index(q))+name][0]+data_iterator['variance_ada'+str(starters.index(q))+name][0]
                    p_pred=total_sum/data_iterator['variance_pred'+str(starters.index(q))+name][0]
                    p_gbrt=total_sum/data_iterator['variance_gbrt'+str(starters.index(q))+name][0]
                    p_ada=total_sum/data_iterator['variance_ada'+str(starters.index(q))+name][0]
                    dividend=p_pred+p_gbrt+p_ada
                    data.loc[0,'prediction_final_'+name+str(starters.index(q))]=(p_pred*data['predictions_'+name+str(starters.index(q))][0]+p_gbrt*data['predictions_gbrt_'+name+str(starters.index(q))][0]+p_ada*data['predictions_ada_'+name+str(starters.index(q))][0])/dividend

    file1=open('/home/p/PycharmProjects/project1/models_good_scaled/predictions1'+'.txt','w')
    file1.write(data.to_csv())
    file1.close()


def predictor(data,data_labels,starters):
    valori=data_labels.columns.values
    for q in starters:
        for h in range(0,len(valori)):
            name=valori[h]
            for i in range(0,len(data)):
                if data[name+str(starters.index(q))][i]!=np.nan:
                    data.loc[i,'pred_dif']=(data['predictions_'+name+str(starters.index(q))][i]-data[name+str(starters.index(q))][i])**2
                    data.loc[i,'gbrt_dif']=(data['predictions_gbrt_'+name+str(starters.index(q))][i]-data[name+str(starters.index(q))][i])**2
                    data.loc[i,'ada_dif']=(data['predictions_ada_'+name+str(starters.index(q))][i]-data[name+str(starters.index(q))][i])**2

                    total_pred=0
                    total_gbrt=0
                    total_ada=0
                for j in range(0,len(data)):
                    total_pred+=data['pred_dif'][j]
                    divid_pred=j+1
                    data.loc[j,'variance_pred']=math.sqrt(total_pred/divid_pred)
                for j in range(0,len(data)):
                    total_gbrt+=data['gbrt_dif'][j]
                    divid_gbrt=j+1
                    data.loc[j,'variance_gbrt']=math.sqrt(total_gbrt/divid_gbrt)
                for j in range(0,len(data)):
                    total_ada+=data['ada_dif'][j]
                    divid_ada=j+1
                    data.loc[j,'variance_ada']=math.sqrt(total_ada/divid_ada)

                for k in range(0,len(data)):
                    total_sum=data['variance_pred'][k]+data['variance_gbrt'][k]+data['variance_ada'][k]
                    p_pred=total_sum/data['variance_pred'][k]
                    p_gbrt=total_sum/data['variance_gbrt'][k]
                    p_ada=total_sum/data['variance_ada'][k]
                    dividend=p_pred+p_gbrt+p_ada
                    data.loc[k,'prediction_final_'+name+str(starters.index(q))]=(p_pred*data['predictions_'+name+str(starters.index(q))][k]+p_gbrt*data['predictions_gbrt_'+name+str(starters.index(q))][k]+p_ada*data['predictions_ada_'+name+str(starters.index(q))][k])/dividend

            for i in range(0,len(data)):
                if data[name+str(starters.index(q))][i]==np.nan:

                    data.loc[k,'prediction_final_'+name+str(starters.index(q))]=(p_pred*data['predictions_'+name+str(starters.index(q))][k]+p_gbrt*data['predictions_gbrt_'+name+str(starters.index(q))][k]+p_ada*data['predictions_ada_'+name+str(starters.index(q))][k])/dividend


    file1=open('/home/p/PycharmProjects/project1/models_good_scaled/predictions1'+'.txt','w')
    file1.write(data.to_csv())
    file1.close()

def delayer(data,data_labels,start_window,window,starters,lista):
    dataframe_delay=pd.DataFrame()
    for q in starters:
        for l in range(0,len(lista)):
             name=lista[l]
             delay=pd.DataFrame()
             lis=[]
             for z in range(start_window, window+1):
                 data_loop=data.copy()
                 data_loop['prediction_final_'+name+'copy']=data_loop['prediction_final_'+name+str(q)].copy()
                 data_loop['prediction_final_'+name+'copy']=data_loop['prediction_final_'+name+str(q)].shift(-z)
                 data_loop=data_loop.dropna()
                 for j in range(0,len(data_loop)):
                     delay.loc[j,'msroot']=(data_loop['prediction_final_'+name+'copy'][j]-data_loop[name+str(q)][j])**2
                     total=0
                 for k in range(0,len(data_loop)):
                    total+=delay['msroot'][k]
                 divid_pred=len(data_loop)
                 variance=math.sqrt(total/divid_pred)
                 lis.append([z,variance])
             delayed=min(lis,key=lambda x:x[1])
             lis.remove(delayed)
             delayed1=min(lis,key=lambda x:x[1])
             lis.remove(delayed1)
             delayed2=min(lis,key=lambda x:x[1])
             dataframe_delay[name+str(q)]=pd.Series()
             dataframe_delay.loc[0,'prediction_final_'+name+str(q)]=delayed[0]
             dataframe_delay.loc[1,'prediction_final_'+name+str(q)]=delayed1[0]
             dataframe_delay.loc[2,'prediction_final_'+name+str(q)]=delayed2[0]
             dataframe_delay['prediction_final_'+name+str(q)]=dataframe_delay['prediction_final_'+name+str(q)].astype(int)
             dataframe_delay.loc[0,name+str(q)]=0
             dataframe_delay.loc[1,name+str(q)]=0
             dataframe_delay.loc[2,name+str(q)]=0
             dataframe_delay[name+str(q)]=dataframe_delay[name+str(q)].astype(int)
    return dataframe_delay


def prevision(data,data_pred,data_pred1,attribute,prediction_attribute,window,delay_data,value):  #prediction_msq,
    delay=delay_data[prediction_attribute][0]
    delay1=delay_data[prediction_attribute][1]
    delay2=delay_data[prediction_attribute][2]
    print(delay)
    print(delay1)
    print(delay2)
    window_=pd.DataFrame()
    window_.loc[0,'0']=window
    window_.loc[0,'1']=2*window
    window_.loc[0,'2']=4*window
    window_.loc[0,'3']=6*window
    window_.loc[0,'4']=10*window
    window_.loc[0,'5']=15*window
    set=[window_['0'][0].astype(int),window_['1'][0].astype(int),window_['2'][0].astype(int),window_['3'][0].astype(int),window_['4'][0].astype(int),window_['5'][0].astype(int)]
    product=1
    print(set)
    for i in range(0,len(set)):
        product*=set[i]
    print(product)
    dividend=0
    for i in range(1,len(set)+1):
        dividend+=i
    print(dividend)
    coefficient=0
    for i in range(0,len(set)):
        coefficient+=(len(set)-i)*product/window_['{0}'.format(i)][0]
    for i in range(value,value+1):

        print(data['close_'+str(start)][value])
        summation=pd.DataFrame()
        summation.loc[0,'0']=0
        summation.loc[0,'1']=0
        summation.loc[0,'2']=0
        summation.loc[0,'3']=0
        summation.loc[0,'4']=0
        summation.loc[0,'5']=0
        for j in range(0,len(set)):
            for k in range(0,set[j]-1):
                summation.loc[0,'{0}'.format(j)]+=data[attribute][i-k]
            summation['{0}'.format(j)]=((len(set)-j)*product/set[j])*summation['{0}'.format(j)]
        final_summation=0
        for l in range(0,len(set)):
            final_summation+=summation['{0}'.format(l)][0]
        forward=0
        counter=0
        margin=int(round((window/2)+1))

        o=21 #21 is the last data of a window of delaymax (=20) + 1 last value
        if (margin-2-delay)>0:
            data_pred1.loc[i+forward,'close_pred_'+prediction_attribute]=(dividend*product*data_pred[prediction_attribute][o-(margin-2-delay)]-final_summation)/(coefficient)
        elif (counter==0) and (margin-2-delay)==0:
            data_pred1.loc[i+forward,'close_pred_'+prediction_attribute]=(dividend*product*data_pred[prediction_attribute][o-(margin-2-delay)]-final_summation)/(coefficient)
            counter+=1
        elif (counter==0) and (margin-2-delay)<0:
            data_pred1.loc[i+forward,'close_pred_'+prediction_attribute]=(dividend*product*data_pred[prediction_attribute][o]-final_summation)/(coefficient)
            counter+=1
        if (margin-2-delay1)>0:
            data_pred1.loc[i+forward,'close_pred1_'+prediction_attribute]=(dividend*product*data_pred[prediction_attribute][o-(margin-2-delay1)]-final_summation)/(coefficient)
        elif (counter==0) and (margin-2-delay1)==0:
            data_pred1.loc[i+forward,'close_pred1_'+prediction_attribute]=(dividend*product*data_pred[prediction_attribute][o-(margin-2-delay1)]-final_summation)/(coefficient)
            counter+=1
        elif (counter==0) and (margin-2-delay1)<0:
            data_pred1.loc[i+forward,'close_pred1_'+prediction_attribute]=(dividend*product*data_pred[prediction_attribute][o]-final_summation)/(coefficient)
            counter+=1
        if (margin-2-delay2)>0:
            data_pred1.loc[i+forward,'close_pred2_'+prediction_attribute]=(dividend*product*data_pred[prediction_attribute][o-(margin-2-delay2)]-final_summation)/(coefficient)
        elif (counter==0) and (margin-2-delay2)==0:
            data_pred1.loc[i+forward,'close_pred2_'+prediction_attribute]=(dividend*product*data_pred[prediction_attribute][o-(margin-2-delay2)]-final_summation)/(coefficient)
            counter+=1
        elif (counter==0) and (margin-2-delay2<0):
            data_pred1.loc[i+forward,'close_pred2_'+prediction_attribute]=(dividend*product*data_pred[prediction_attribute][o]-final_summation)/(coefficient)
            counter+=1

    return data_pred1

def stack_prevision(data,data_pred,data_pred1,loop_set,delay_data,starters,value):
    data_pred1=pd.DataFrame()
    for q in starters:
        for i in loop_set:
            lon=15*i
            prevision(data,data_pred,data_pred1,'close_'+str(q),'prediction_final_totalmovavg_predictclose'+str(i)+'_'+str(lon)+str(starters.index(q)),i,delay_data,value)
    data_c=data['close_'+str(starters[len(starters)-1])].iloc[value:value+1]
    frame=[data_pred1,data_c]
    data_pred1=pd.concat(frame,axis=1)
    return data_pred1


def price(symbol, comparison_symbols=['USD'], exchange=''):
    url = 'https://min-api.cryptocompare.com/data/price?fsym={}&tsyms={}'\
            .format(symbol.upper(), ','.join(comparison_symbols).upper())
    if exchange:
        url += '&e={}'.format(exchange)
    page = requests.get(url)
    data = page.json()
    return data


def minute_price_historical(symbol, comparison_symbol, limit, aggregate, exchange=''):
    url = 'https://min-api.cryptocompare.com/data/histominute?fsym={}&tsym={}&limit={}&aggregate={}'\
            .format(symbol.upper(), comparison_symbol.upper(), limit, aggregate)
    if exchange:
        url += '&e={}'.format(exchange)
    page = requests.get(url)
    data = page.json()['Data']
    df = pd.DataFrame(data)
    df['timestamp'] = [datetime.datetime.fromtimestamp(d) for d in df.time]
    return df




def datasets(ticker, comparison_symb, minute_number_of_data, minute_time_delta):

    mdf = minute_price_historical(ticker, comparison_symb, minute_number_of_data, minute_time_delta)
    mtime=str(mdf.timestamp.max())
    m=[]
    for i in range(0,len(mdf["timestamp"])):
        m.append((str(mdf.timestamp[i])[0:4])+(str(mdf.timestamp[i])[5:7])+(str(mdf.timestamp[i])[8:10])+(str(mdf.timestamp[i])[11:13])+(str(mdf.timestamp[i])[14:16])+(str(mdf.timestamp[i])[17:19]))
    m=np.array(m)
    mdf1=pd.DataFrame({'close':mdf['close'],'high':mdf['high'],'low':mdf['low'],'open':mdf['open'],'time':mdf['time'],'volumefrom':mdf['volumefrom'],'volumeto':mdf['volumeto'],'date':m})
    return mdf1

def data_creator(ticker,comparison_symb,minute_number_of_data,minute_time_delta):

    ticker='BTC'
    comparison_symb='EUR' #*****************['BTC', 'ETH', 'USD']
    minute_number_of_data=minute_number_of_data
    hour_time_delta = 1 # Bar width in hours --- 24
    minute_time_delta = minute_time_delta # Bar width in minutes
    mdf=datasets(ticker, comparison_symb, minute_number_of_data, minute_time_delta)
    return mdf
    
    
def main():

    file=pd.read_csv('/home/p/PycharmProjects/project1/file_machine_learning1.txt')
    data1=pd.DataFrame(file)
    data1=data1.dropna()
    data1=data1.reset_index()
    data1=data1.drop(['Unnamed: 0'],axis=1)
    data1=data1.drop(['index'],axis=1)

    data_c=data1
    data1=data_c.iloc[:int(9*len(data_c)/10),:]
    data1_2=data_c.iloc[int(9*len(data_c)/10):,:]
    data1_2=data1_2.reset_index()
    data1_2=data1_2.drop(['index'],axis=1)


    loop_set=[3,5,7,10,15]
    attribute='close'
    a=splitting_cleaning(data1,attribute,'slope3_close',loop_set,0.2,0.3,3)


    test_lay=a[0]
    test_labels_lay=a[1]
    test_labels_lay=pd.DataFrame(test_labels_lay)
    test=a[2]
    test_labels=a[3]

    train={}
    train_lab={}

    for i in xrange(0,len(a)-4,2):
        p=int(i/2)
        train['{0}'.format(p)]=a[i+4]
        train_lab['{0}'.format(p)]=a[i+5]
        train_lab['{0}'.format(p)]=pd.DataFrame(train_lab['{0}'.format(p)])
        #train_lab['{0}'.format(p)]=train_lab['{0}'.format(p)].reset_index('Unnamed: 0')
        #train_lab['{0}'.format(p)]=train_lab['{0}'.format(p)].drop(['index'],axis=1)


    lista=list(train_lab['0'].columns.values)
    index=int((len(a)-4)/2)

    from sklearn.preprocessing import StandardScaler
    scaler=StandardScaler()
    scalert=StandardScaler()

    #scaling labels

    scaler_=scaler.fit(train_lab['0'])
    test_labels=pd.DataFrame(scaler_.transform(test_labels),columns=test_labels.columns)
    test_labels_lay=pd.DataFrame(scaler_.transform(test_labels_lay),columns=test_labels_lay.columns)
    for j in range(0,index):
        if len(train_lab[str(j)])!=0:
            train_lab[str(j)]=pd.DataFrame(scaler_.transform(train_lab[str(j)]),columns=train_lab[str(j)].columns)


    #scaling train
    scaler1_=scalert.fit(train['0'])
    test=pd.DataFrame(scaler1_.transform(test),columns=test.columns)
    test_lay=pd.DataFrame(scaler1_.transform(test_lay),columns=test_lay.columns)
    for j in range(0,index):
            train[str(j)]=pd.DataFrame(scaler1_.transform(train[str(j)]),columns=train[str(j)].columns)

    ##############################################################################

    #machine learning

    #Random forest
    for i in range(0,len(lista)):
        name=str(lista[i])
        train_lab_sing={}
        for j in range(0,index):
            train_lab_sing['{0}'.format(j)]=train_lab['{0}'.format(j)][name]


    #Ada Boost

        from sklearn.ensemble import AdaBoostRegressor
        from sklearn.tree import DecisionTreeRegressor
        ada_reg=AdaBoostRegressor(
            DecisionTreeRegressor(max_depth=8),n_estimators=70, learning_rate=0.3,)
        adamodel=ada_reg.fit(train['0'],train_lab_sing['0'])

        from sklearn.externals import joblib
        joblib.dump(adamodel,'ada_'+name)

        #Gradient Boosting

        from sklearn.ensemble import GradientBoostingRegressor
        from sklearn.metrics import mean_squared_error
        from sklearn.model_selection import train_test_split

        Xtr,Xval,Ytr,Yval=train_test_split(train['0'],train_lab_sing['0'],test_size=0.33,random_state=42)

        gbrt=GradientBoostingRegressor(max_depth=7,n_estimators=70,learning_rate=0.1)
        gbrt.fit(Xtr,Ytr)
        errors=[mean_squared_error(Yval,predictions)
            for predictions in gbrt.staged_predict(Xval)]
        bst_n_estimators=np.argmin(errors)
        gbrt_best=GradientBoostingRegressor(max_depth=7,n_estimators=bst_n_estimators)
        modelgbrt=gbrt_best.fit(Xtr,Ytr)

        from sklearn.externals import joblib
        joblib.dump(modelgbrt,'gbrt_'+name)



        #Stacking

        from sklearn.externals import joblib

        gbrt=joblib.load('gbrt_'+name)
        ada=joblib.load('ada_'+name)


        gbrt1=gbrt.predict(train['1']) #puoi mettere 2 al gbrt e 1 agli altri, ma comunque...
        ada1=ada.predict(train['1'])

        stacking_blen=pd.DataFrame({'gbrt':gbrt1,'ada':ada1})




        from sklearn.metrics import mean_squared_error


        mse_blen_gbrt=mean_squared_error(pd.DataFrame(stacking_blen['gbrt']),pd.DataFrame(train_lab_sing['1']))
        rmse_blen_gbrt=np.sqrt(mse_blen_gbrt)
        print(rmse_blen_gbrt)

        mse_blen_ada=mean_squared_error(pd.DataFrame(stacking_blen['ada']),pd.DataFrame(train_lab_sing['1']))
        rmse_blen_ada=np.sqrt(mse_blen_ada)
        print(rmse_blen_ada)

        from sklearn.model_selection import GridSearchCV
        from sklearn.ensemble import RandomForestRegressor

        stacking_blen=pd.DataFrame(stacking_blen)
        length_blen=int(len(stacking_blen.columns.values))
        param_grid_final = [
                {'n_estimators': [70], 'max_features':[length_blen],'max_depth':[8],},
            #{'bootstrap': [False],'n_estimators': [30,60,120], 'max_features': [10,20]},
            ]
        forest_reg_final = RandomForestRegressor()
        grid_search_final = GridSearchCV(forest_reg_final,param_grid_final,cv=5,scoring='neg_mean_squared_error')
        model_forest_final=grid_search_final.fit(stacking_blen,train_lab_sing['1'])

        from sklearn.externals import joblib
        joblib.dump(model_forest_final,name)

if __name__=='__main__':
    main()
